{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c330cf-90f2-428d-b5c3-aa2ba4a4d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import prune\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f2cfb5-f2ce-4fa3-83e6-09395c7ccfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad202-686c-4860-8547-06d7b92b1402",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4144ac-d34c-4583-8917-c7f45321810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91332b8-7352-4dec-b020-aaceba6e9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595e7d5-14e1-4942-be25-19a60bcd6a44",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02daef98-a1d7-4f2a-a321-5ad02cf5c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       1,728\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Conv2d: 2-4                       36,864\n",
      "|    └─BatchNorm2d: 2-5                  128\n",
      "|    └─ReLU: 2-6                         --\n",
      "|    └─MaxPool2d: 2-7                    --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Conv2d: 2-8                       73,728\n",
      "|    └─BatchNorm2d: 2-9                  256\n",
      "|    └─ReLU: 2-10                        --\n",
      "|    └─Conv2d: 2-11                      147,456\n",
      "|    └─BatchNorm2d: 2-12                 256\n",
      "|    └─ReLU: 2-13                        --\n",
      "|    └─MaxPool2d: 2-14                   --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-15                      294,912\n",
      "|    └─BatchNorm2d: 2-16                 512\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─Conv2d: 2-18                      589,824\n",
      "|    └─BatchNorm2d: 2-19                 512\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─MaxPool2d: 2-21                   --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Conv2d: 2-22                      1,179,648\n",
      "|    └─BatchNorm2d: 2-23                 1,024\n",
      "|    └─ReLU: 2-24                        --\n",
      "|    └─Conv2d: 2-25                      2,359,296\n",
      "|    └─BatchNorm2d: 2-26                 1,024\n",
      "|    └─ReLU: 2-27                        --\n",
      "|    └─MaxPool2d: 2-28                   --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Linear: 2-29                      4,196,352\n",
      "|    └─ReLU: 2-30                        --\n",
      "|    └─Linear: 2-31                      2,098,176\n",
      "|    └─ReLU: 2-32                        --\n",
      "|    └─Linear: 2-33                      10,250\n",
      "=================================================================\n",
      "Total params: 10,992,074\n",
      "Trainable params: 10,992,074\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = self._make_conv_block(in_ch=3,   out_ch=64)\n",
    "        self.block2 = self._make_conv_block(in_ch=64,  out_ch=128)\n",
    "        self.block3 = self._make_conv_block(in_ch=128, out_ch=256)\n",
    "        self.block4 = self._make_conv_block(in_ch=256, out_ch=512)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 2048),  # из 512x2x2 -> 2048\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_conv_block(self, in_ch, out_ch):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x) \n",
    "        x = self.block2(x)  \n",
    "        x = self.block3(x)  \n",
    "        x = self.block4(x) \n",
    "\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = CNN(num_classes=10)\n",
    "model.load_state_dict(torch.load('./model.pt', weights_only=True))\n",
    "\n",
    "_ = summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=DEVICE, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078a280-003d-4e62-96bd-914482ab756d",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfd213-b537-452a-9534-cf75df3f2692",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384fd27c-1732-47b1-9a5e-8de45551f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc6ea9-4943-4b2d-b459-42248530631e",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0defded-216d-4f36-ac4a-cf6fd932fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=0)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=1, dim=1)\n",
    "\n",
    "def remove_pruning_masks(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "pruned_model = CNN(num_classes=10)\n",
    "pruned_model.load_state_dict(torch.load('./model.pt', weights_only=True))\n",
    "apply_pruning(pruned_model)\n",
    "remove_pruning_masks(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80bd0a1-d26f-4f0a-8b6f-ce93aaa537af",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e781aa-3333-4006-93d9-12dd21cf3f42",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb8faf5-7871-4239-8035-1a0d5a839f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 17.895MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in quantized_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in quantized_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('Model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4818ef9c-ec1a-423c-9333-e9e511db2d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 4687296\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "params = count_parameters(quantized_model)\n",
    "print(\"Params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685ae859-2a7e-4b82-8954-05d804d75191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg inference time: 1.9107 ms\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "num_samples = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    output = quantized_model(inp)\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932edeae-319c-45ee-829b-264a5bdd6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.98\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_, y_ in val_loader:\n",
    "        outputs = quantized_model(X_)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_).sum().item()\n",
    "        total += y_.size(0)\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8318a6a-a3bd-4ae1-9b21-072c600eebc6",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11a26e-5f7a-456a-b34c-1091a2e403a5",
   "metadata": {},
   "source": [
    "### Размеры остались прежними, потому что torch не пересоздает архитектуру "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0313f900-3f43-4301-be8d-c9234b8727df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 41.946MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in pruned_model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in pruned_model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('Model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9044053-ac49-496a-8e4f-2c90ac3fffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 10992074\n"
     ]
    }
   ],
   "source": [
    "params = count_parameters(pruned_model)\n",
    "print(\"Params:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28b0155d-d7bb-4a9a-a1c2-13d56833e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Avg inference time: 2.3608 ms\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "num_samples = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    output = pruned_model(inp)\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'CPU Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "635c0703-4395-4b4e-9b35-bebaa12a2690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Avg inference time: 1.1596 ms\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "\n",
    "pruned_model.to(DEVICE)\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    output = pruned_model(inp.to(DEVICE))\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'GPU Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c038af-6f75-46f1-b3d3-605044b9ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.66\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_, y_ in val_loader:\n",
    "        X_, y_ = X_.to(DEVICE), y_.to(DEVICE)\n",
    "        outputs = pruned_model(X_)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_).sum().item()\n",
    "        total += y_.size(0)\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49aa3ac-f4cf-42c6-a386-7de371042149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
