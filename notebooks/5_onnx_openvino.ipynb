{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30c330cf-90f2-428d-b5c3-aa2ba4a4d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from openvino.tools import ovc\n",
    "from openvino.runtime import serialize, Core\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f2cfb5-f2ce-4fa3-83e6-09395c7ccfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806ad202-686c-4860-8547-06d7b92b1402",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4144ac-d34c-4583-8917-c7f45321810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                std=[0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91332b8-7352-4dec-b020-aaceba6e9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595e7d5-14e1-4942-be25-19a60bcd6a44",
   "metadata": {},
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02daef98-a1d7-4f2a-a321-5ad02cf5c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       1,728\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Conv2d: 2-4                       36,864\n",
      "|    └─BatchNorm2d: 2-5                  128\n",
      "|    └─ReLU: 2-6                         --\n",
      "|    └─MaxPool2d: 2-7                    --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Conv2d: 2-8                       73,728\n",
      "|    └─BatchNorm2d: 2-9                  256\n",
      "|    └─ReLU: 2-10                        --\n",
      "|    └─Conv2d: 2-11                      147,456\n",
      "|    └─BatchNorm2d: 2-12                 256\n",
      "|    └─ReLU: 2-13                        --\n",
      "|    └─MaxPool2d: 2-14                   --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-15                      294,912\n",
      "|    └─BatchNorm2d: 2-16                 512\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─Conv2d: 2-18                      589,824\n",
      "|    └─BatchNorm2d: 2-19                 512\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─MaxPool2d: 2-21                   --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Conv2d: 2-22                      1,179,648\n",
      "|    └─BatchNorm2d: 2-23                 1,024\n",
      "|    └─ReLU: 2-24                        --\n",
      "|    └─Conv2d: 2-25                      2,359,296\n",
      "|    └─BatchNorm2d: 2-26                 1,024\n",
      "|    └─ReLU: 2-27                        --\n",
      "|    └─MaxPool2d: 2-28                   --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Linear: 2-29                      4,196,352\n",
      "|    └─ReLU: 2-30                        --\n",
      "|    └─Linear: 2-31                      2,098,176\n",
      "|    └─ReLU: 2-32                        --\n",
      "|    └─Linear: 2-33                      10,250\n",
      "=================================================================\n",
      "Total params: 10,992,074\n",
      "Trainable params: 10,992,074\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.block1 = self._make_conv_block(in_ch=3,   out_ch=64)\n",
    "        self.block2 = self._make_conv_block(in_ch=64,  out_ch=128)\n",
    "        self.block3 = self._make_conv_block(in_ch=128, out_ch=256)\n",
    "        self.block4 = self._make_conv_block(in_ch=256, out_ch=512)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 2048),  # из 512x2x2 -> 2048\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_conv_block(self, in_ch, out_ch):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x) \n",
    "        x = self.block2(x)  \n",
    "        x = self.block3(x)  \n",
    "        x = self.block4(x) \n",
    "\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = CNN(num_classes=10).to(DEVICE)\n",
    "model.load_state_dict(torch.load('./model.pt', weights_only=True))\n",
    "\n",
    "_ = summary(model, input_size=(BATCH_SIZE, 3, 32, 32), device=DEVICE, depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f850e9-c680-4b72-b2c0-308bcba02241",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dace4-9cab-4d4a-941b-db66902d49e5",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f332db9-8842-4116-80f5-aff6a3ecc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32).to(DEVICE)\n",
    "torch.onnx.export(model, dummy_input, 'model.onnx', opset_version=14, export_params=True, input_names=[\"input\"], \n",
    "                  output_names=[\"output\"], dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e4acf-e38f-4167-9ce8-bf78b37d1054",
   "metadata": {},
   "source": [
    "## OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e6a544-494f-4fc1-a0d5-51c43b91aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ovc.convert_model('model.onnx')\n",
    "\n",
    "os.makedirs(\"openvino_model\", exist_ok=True)\n",
    "serialize(ov_model, \"openvino_model/model.xml\")\n",
    "openvino_bin_path = \"openvino_model/model.bin\"\n",
    "openvino_xml_path = \"openvino_model/model.xml\"\n",
    "\n",
    "# openvino_model_size = os.path.getsize(openvino_bin_path) / (1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8888141-d493-4257-a9a5-b34e8d3357fc",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351576f-472c-41b1-8419-36267e9eff5e",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c7d81fd-085f-4c81-bc27-4aea29a192d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.92947006225586"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize('model.onnx') / (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28b0155d-d7bb-4a9a-a1c2-13d56833e23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Avg inference time: 1.2587 ms\n"
     ]
    }
   ],
   "source": [
    "onnx_session = ort.InferenceSession('model.onnx', providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "test_images, test_labels = next(iter(val_loader))\n",
    "test_images = test_images[:1]  \n",
    "test_images_np = test_images.numpy() \n",
    "onnx_inputs = {\"input\": test_images_np}\n",
    "onnx_out = onnx_session.run([\"output\"], onnx_inputs)[0]\n",
    "\n",
    "num_samples = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    onnx_out = onnx_session.run([\"output\"], onnx_inputs)[0]\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'CPU Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "158cd044-428f-4351-aad0-9da355a54f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Avg inference time: 0.4011 ms\n"
     ]
    }
   ],
   "source": [
    "onnx_session = ort.InferenceSession('model.onnx', providers=[\"CUDAExecutionProvider\"])\n",
    "\n",
    "test_images, test_labels = next(iter(val_loader))\n",
    "test_images = test_images[:1]  \n",
    "test_images_np = test_images.numpy() \n",
    "onnx_inputs = {\"input\": test_images_np}\n",
    "onnx_out = onnx_session.run([\"output\"], onnx_inputs)[0]\n",
    "\n",
    "num_samples = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    onnx_out = onnx_session.run([\"output\"], onnx_inputs)[0]\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'CPU Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd0b7992-f1c1-49fa-be12-bf0905993d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.05\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for X_, y_ in val_loader:\n",
    "    onnx_inputs = {\"input\": np.array(X_)}\n",
    "    outputs = onnx_session.run([\"output\"], onnx_inputs)[0]\n",
    "    _, predicted = torch.max(torch.tensor(outputs), 1)\n",
    "    correct += (predicted == y_).sum().item()\n",
    "    total += y_.size(0)\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05aa90c-e1ed-4879-9b73-783647f4786b",
   "metadata": {},
   "source": [
    "## OpenVino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7874fbeb-5ef4-4857-b764-4a5fec8ba456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.95438098907471"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([os.path.getsize(f'./openvino_model/{fn}') / (1024**2) for fn in os.listdir('./openvino_model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56c4d77a-0dae-450e-ba30-b0bf82659b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Avg inference time: 1.3848 ms\n"
     ]
    }
   ],
   "source": [
    "core = Core()\n",
    "model = core.read_model('openvino_model/model.xml')\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "output_any_name = compiled_model.outputs[0]\n",
    "\n",
    "test_images_ov = test_images_np.astype(np.float32)\n",
    "res_ov = compiled_model([test_images_ov])[output_any_name]\n",
    "\n",
    "num_samples = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_samples):\n",
    "    ov_out = compiled_model([test_images_ov])[output_any_name]\n",
    "end_time = time.time()\n",
    "\n",
    "infer_time = ((end_time - start_time) / num_samples) * 1000\n",
    "print(f'CPU Avg inference time: {infer_time:.4f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9462ab86-7e0c-465c-9d92-105b99246edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.05\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for X_, y_ in val_loader:\n",
    "    X_ = X_.cpu().numpy().astype(np.float32)\n",
    "    outputs = compiled_model([X_])[output_any_name]\n",
    "    _, predicted = torch.max(torch.tensor(outputs), 1)\n",
    "    correct += (predicted == y_).sum().item()\n",
    "    total += y_.size(0)\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1224a37-8d9b-4a02-b056-bdb4ea027592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
